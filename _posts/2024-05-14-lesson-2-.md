---
title: "Lesson 2: Neural Network Basics"
date: "2024-05-14"
categories:
  - Blog
  - Lesson
tags:
  - Basic
  - Deeplearning&Neuralnetwork
  - Logistic Regression
  - Vectorization
---

# 2.0 Binary Classification (Problem)
Binary Classification only produces two prediction types (output, y), normally labelled as (0, 1).
![image](https://github.com/yitkhee17/Idea/assets/135970016/fe15fb63-13a2-491c-aa09-24d79303d62e)
## 2.0.1 Training Notation
![image](https://github.com/yitkhee17/Idea/assets/135970016/01eb9c4b-33e9-4dfd-ae2c-74fc229d9430)

# 2.1 Logistic Regression （Problem Solving Algorithm）
An algorithm designed for binary classification problems.

Given x, predict y hat, for P(y = 1 | x) [given x, find the prob of y equal to 1].
![image](https://github.com/yitkhee17/Idea/assets/135970016/f62e3f3e-46ed-4e0e-ac8b-2ac9056a6082)

![image](https://github.com/yitkhee17/Idea/assets/135970016/b4e4fa5e-788c-4c07-bb8d-2fe43d2e0128)

A cost function should be defined to change the w and b parameters.

## 2.1.1 Logistic Regression Cost Function
By now, we have
![image](https://github.com/yitkhee17/Idea/assets/135970016/96c50fe6-f920-4969-8a41-69f3a0a437a1)
Given X, we want to predict Y. For the first training sample, x(1): 
![image](https://github.com/yitkhee17/Idea/assets/135970016/9b21106c-1ad8-4a99-b88d-18d3aa9eb5a9)

## 2.1.2 Loss (error) function: 
measure the performance (when y hat is predicted, the distance between y hat and the true label records loss of information). Normally, calculated by square error.
![image](https://github.com/yitkhee17/Idea/assets/135970016/25d9b655-0d45-42f9-8fee-b47f77a07923)
The lower square error means better prediction. However logistic regression does not use this, as gradient descent might not be able to find the global optimal (best param values).

## 2.1.3 Loss function for logistic regression:
 ![image](https://github.com/yitkhee17/Idea/assets/135970016/ed716fb4-1d29-4975-9278-63557b0ab4e2)

Lower loss values also indicate better prediction. It would provide a convex optimization problem. 

Single loss value measures how well the prediction is done on a single training set (x1,y1)

## 2.1.4 Cost function, j:
Now, let's define a cost function, measuring how well prediction is done on the entire training set. It is applied to the parameter (w,b), 
![image](https://github.com/yitkhee17/Idea/assets/135970016/c0f3f0ea-8b8c-4580-b0b6-20f8019dc3c1)

In the end, we want to find the value of (w,b) that minimizes the cost function (whole training set). With the y-axis as a loss value, our target is the minimum point (w,b) with the least loss.
![image](https://github.com/yitkhee17/Idea/assets/135970016/2a3db62e-0312-490d-8612-e79641aa757a)
From this illustration, the cost function, j is a convex function (bowl shape, only one global optimal value). For non-convex, different local optimal values are present in the graph, illustrating a wavy shape.
A logistic regression algorithm can be viewed as a small neural network, let's see. 

# 2.2 Gradient Descent (Solution Enhancing Algorithm)
to train and learn the best working (w,b) parameters on the training set.

Optimizing w and b:
1. initial w and b with initialization method (all works for logistic regression, as we eventually go to that only global optimal point)
2. [Repeated steps] Take a step in the steepest downhill direction iteratively until the nearest global optimal is found.
![image](https://github.com/yitkhee17/Idea/assets/135970016/4781b06a-50b4-4bc1-82b5-c2f7640237c4)
![image](https://github.com/yitkhee17/Idea/assets/135970016/b6d8a7fb-37a7-429a-935f-fea6fa3fb7fb)
![image](https://github.com/yitkhee17/Idea/assets/135970016/5b6ca20a-010f-486b-99ba-c869e4489738)

**Derivatives**

Derivatives are the slope of the function. It shows what happen when the variable a nudge right by 0.001: f(a) would inc by x.xx
1. ![image](https://github.com/yitkhee17/Idea/assets/135970016/2bfdb2bc-d067-4c88-8d6e-1ddf21cea98a) The derivative is calculated by height/width.
2. ![image](https://github.com/yitkhee17/Idea/assets/135970016/63563453-df2b-441f-880e-ead24577154e)
3. ![image](https://github.com/yitkhee17/Idea/assets/135970016/c3d785ed-d027-42ad-9ce8-5c01b50a1070)
4. ![image](https://github.com/yitkhee17/Idea/assets/135970016/576021b3-2af8-4783-96c0-15d05aa68ad5)


# 2.2.1 Computational Graph
1. Forward propagation: to produce output for the function.
2. Backward propagation: to calculate derivatives for the function.

![image](https://github.com/yitkhee17/Idea/assets/135970016/9347d57d-7b2e-43fb-96fc-a2215ff744e8)

# Deravatives with Computational Graph
# Logistic Regression Gradient Descent

# Logistic Regression as a Neural Network

# Python and Vectorization
# Vectorization
# Vectorizing Logistic Regression
# Vectorizing Logistic Regression's Gradient Output
# Python
# Numpy Vectors
# Logistic Regression Cost Function
