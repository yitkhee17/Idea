---
title: "Lesson 2: Neural Network Basics"
date: "2024-05-14"
categories:
  - Blog
  - Lesson
tags:
  - Basic
  - Deeplearning&Neuralnetwork
  - Logistic Regression
  - Vectorization
---

# Binary Classification (Problem)
Binary Classification only produce two type of prediction (output, y), normally labelled as (0, 1).
![image](https://github.com/yitkhee17/Idea/assets/135970016/fe15fb63-13a2-491c-aa09-24d79303d62e)
## Training Notation
![image](https://github.com/yitkhee17/Idea/assets/135970016/01eb9c4b-33e9-4dfd-ae2c-74fc229d9430)

## Logistic Regression （Algorithm）
An algorithm designed for binary classification problems.

Given x, predict y hat, for P(y = 1 | x) [given x, find the prob of y equal to 1].
![image](https://github.com/yitkhee17/Idea/assets/135970016/f62e3f3e-46ed-4e0e-ac8b-2ac9056a6082)

![image](https://github.com/yitkhee17/Idea/assets/135970016/b4e4fa5e-788c-4c07-bb8d-2fe43d2e0128)

To change the w and b parameters, a cost function should be defined.

## Logistic Regression Cost Function
By now, we have
![image](https://github.com/yitkhee17/Idea/assets/135970016/96c50fe6-f920-4969-8a41-69f3a0a437a1)
Given X, we want to predict Y. For the first training sample, x(1): 
![image](https://github.com/yitkhee17/Idea/assets/135970016/9b21106c-1ad8-4a99-b88d-18d3aa9eb5a9)

### Loss (error) function: 
measure the performance (when y hat is predicted, the distance between y hat and the true label records loss of information). Normally, calculated by square error.
![image](https://github.com/yitkhee17/Idea/assets/135970016/25d9b655-0d45-42f9-8fee-b47f77a07923)
Lower square error means better prediction. But logistic regression do not use this, as gradient descent might not able to find the global optimal (best param values).

### Loss function for logistic regression:
 ![image](https://github.com/yitkhee17/Idea/assets/135970016/ed716fb4-1d29-4975-9278-63557b0ab4e2)

Lower loss value also indicate better prediction. It would provide convex optimization problem. 

Single loss value measure how well the prediction is done on single training set (x1,y1)

### Cost function, j:
Now, let's define a cost function, measuring how well prediction is done on entire training set. It is applied to the parameter (w,b), 
![image](https://github.com/yitkhee17/Idea/assets/135970016/c0f3f0ea-8b8c-4580-b0b6-20f8019dc3c1)

At the end, we want to find the value of (w,b) that actually minimize the cost function (whole training set). Having y-axis as loss value, the minimum point (w,b) with least loss is our target.
![image](https://github.com/yitkhee17/Idea/assets/135970016/2a3db62e-0312-490d-8612-e79641aa757a)
From this illustration, the cost function, j is a convex function (bowl shape, only one global optimal value). For non convex, different local optimal values present in the graph, illustrating a wavy shape.
A logistic regression algorithm can be viewed as a small neural network, let's see. 

# Gradient Descent (Algorithm)
to train and learn the (w,b) parameters on the training set.

Optimizing w and b:
1. intial w and b with initialization method (all works for logistic regression, as we eventually go to that one and only global optimal point)
2. [Repeated steps] take a step in steepest downhill direction iteratively until nearest global optimal is found.
![image](https://github.com/yitkhee17/Idea/assets/135970016/4781b06a-50b4-4bc1-82b5-c2f7640237c4)

# Derivatives
# Computational Graph
# Deravatives with Computational Graph
# Logistic Regression Gradient Descent

# Logistic Regression as a Neural Network

# Python and Vectorization
# Vectorization
# Vectorizing Logistic Regression
# Vectorizing Logistic Regression's Gradient Output
# Python
# Numpy Vectors
# Logistic Regression Cost Function
