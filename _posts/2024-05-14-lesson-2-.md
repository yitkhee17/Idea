---
title: "Lesson 2: Neural Network Basics"
date: "2024-05-14"
categories:
  - Blog
  - Lesson
tags:
  - Basic
  - Deeplearning&Neuralnetwork
  - Logistic Regression
  - Vectorization
---

# 2.0 Binary Classification (Problem)
Binary Classification only produces two prediction types (output, y), normally labelled as (0, 1).
![image](https://github.com/yitkhee17/Idea/assets/135970016/fe15fb63-13a2-491c-aa09-24d79303d62e)
## 2.0.1 Training Notation
![image](https://github.com/yitkhee17/Idea/assets/135970016/01eb9c4b-33e9-4dfd-ae2c-74fc229d9430)

# 2.1 Logistic Regression （Problem Solving Algorithm）
An algorithm designed for binary classification problems.

Given x, predict y hat, for P(y = 1 | x) [given x, find the prob of y equal to 1].
![image](https://github.com/yitkhee17/Idea/assets/135970016/f62e3f3e-46ed-4e0e-ac8b-2ac9056a6082)

![image](https://github.com/yitkhee17/Idea/assets/135970016/b4e4fa5e-788c-4c07-bb8d-2fe43d2e0128)

A cost function should be defined to change the w and b parameters.

## 2.1.1 Logistic Regression Cost Function
By now, we have
![image](https://github.com/yitkhee17/Idea/assets/135970016/96c50fe6-f920-4969-8a41-69f3a0a437a1)
Given X, we want to predict Y. For the first training sample, x(1): 
![image](https://github.com/yitkhee17/Idea/assets/135970016/9b21106c-1ad8-4a99-b88d-18d3aa9eb5a9)

## 2.1.2 Loss (error) function: 
measure the performance (when y hat is predicted, the distance between y hat and the true label records loss of information). Normally, calculated by square error.
![image](https://github.com/yitkhee17/Idea/assets/135970016/25d9b655-0d45-42f9-8fee-b47f77a07923)
The lower square error means better prediction. However logistic regression does not use this, as gradient descent might not be able to find the global optimal (best param values).

## 2.1.3 Loss function for logistic regression:
 ![image](https://github.com/yitkhee17/Idea/assets/135970016/ed716fb4-1d29-4975-9278-63557b0ab4e2)

Lower loss values also indicate better prediction. It would provide a convex optimization problem. 

Single loss value measures how well the prediction is done on a single training set (x1,y1)

## 2.1.4 Cost function, j:
Now, let's define a cost function, measuring how well prediction is done on the entire training set. It is applied to the parameter (w,b), 
![image](https://github.com/yitkhee17/Idea/assets/135970016/c0f3f0ea-8b8c-4580-b0b6-20f8019dc3c1)

In the end, we want to find the value of (w,b) that minimizes the cost function (whole training set). With the y-axis as a loss value, our target is the minimum point (w,b) with the least loss.
![image](https://github.com/yitkhee17/Idea/assets/135970016/2a3db62e-0312-490d-8612-e79641aa757a)
From this illustration, the cost function, j is a convex function (bowl shape, only one global optimal value). For non-convex, different local optimal values are present in the graph, illustrating a wavy shape.
A logistic regression algorithm can be viewed as a small neural network, let's see. 

# 2.2 Gradient Descent (Solution Enhancing Algorithm)
to train and learn the best working (w,b) parameters on the training set.

Optimizing w and b:
1. initial w and b with initialization method (all works for logistic regression, as we eventually go to that only global optimal point)
2. [Repeated steps] Take a step in the steepest downhill direction iteratively until the nearest global optimal is found.
![image](https://github.com/yitkhee17/Idea/assets/135970016/4781b06a-50b4-4bc1-82b5-c2f7640237c4)
![image](https://github.com/yitkhee17/Idea/assets/135970016/b6d8a7fb-37a7-429a-935f-fea6fa3fb7fb)
![image](https://github.com/yitkhee17/Idea/assets/135970016/5b6ca20a-010f-486b-99ba-c869e4489738)

**Derivatives**

Derivatives are the slope of the function. It shows what happen when the variable a nudge right by 0.001: f(a) would inc by x.xx
1. ![image](https://github.com/yitkhee17/Idea/assets/135970016/2bfdb2bc-d067-4c88-8d6e-1ddf21cea98a) The derivative is calculated by height/width.
2. ![image](https://github.com/yitkhee17/Idea/assets/135970016/63563453-df2b-441f-880e-ead24577154e)
![image](https://github.com/yitkhee17/Idea/assets/135970016/576021b3-2af8-4783-96c0-15d05aa68ad5)


# 2.2.1 Computational Graph
Computation of Neural Network:
1. Forward propagation: to produce output for the function.
2. Backward propagation: to calculate derivatives for the function.

Eg. Compute a function: J(a,b,c) = 3(a+bc), let a=5, b=3, c=2.

We knew, logically, it needed to calculate the multiplication of [bc] before the addition of the bracket and finally multiply them by 3. This involved a total of 3 steps to **produce the output**.
![image](https://github.com/yitkhee17/Idea/assets/135970016/9347d57d-7b2e-43fb-96fc-a2215ff744e8)

For **calculating the derivatives**, 
1. let's compute the derivative of J to v: 11.001/33.003 = 3
2. let's compute the derivative of J to a:![image](https://github.com/yitkhee17/Idea/assets/135970016/9ced7a4b-e25c-46c5-8439-d7ecc86ff5bc)
3. let's compute the derivative of J to u:![image](https://github.com/yitkhee17/Idea/assets/135970016/4d2b8086-6fea-4b2b-9b72-cc925b2c741a)
4. let's compute the derivative of J to b:![image](https://github.com/yitkhee17/Idea/assets/135970016/c935ded2-c7e5-4d12-b28e-d3dff62ba2ff)
5. let's compute the derivative of J to c:![image](https://github.com/yitkhee17/Idea/assets/135970016/25927dec-f6f9-4561-9c13-067b36460203)


# 2.3 Gradient Descent for Logistic Regression 
Computational Graph for Logistic Regression Derivatives:
![image](https://github.com/yitkhee17/Idea/assets/135970016/c920663f-caf7-43ce-a43b-724c11a6d40c)

Calculating from backward, 
![image](https://github.com/yitkhee17/Idea/assets/135970016/83c9c971-2507-4d20-ab78-5fb0172514d0)
![image](https://github.com/yitkhee17/Idea/assets/135970016/ae4eacca-f338-4432-9b39-bdb2c3a0a86d)

Gradient Descent on m examples:
![image](https://github.com/yitkhee17/Idea/assets/135970016/50a41462-0814-4bac-b078-6a7ed6bf991f)

** For loop slow down the computation -> go for vectorization
